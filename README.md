# BigData
 Assignments worked in Big Data 
The Assignment 1 folder focuses on data cleaning and the utilization of the MapReduce and Dusk functions for big data processing. It contains the following files:

data_cleaning.py: This file contains code for cleaning and preprocessing large datasets. It includes techniques such as data normalization, handling missing values, and removing outliers.

mapreduce_functions.py: Here, you will find code implementations of the MapReduce function. These functions help in parallelizing the processing of large datasets across multiple nodes in a distributed computing environment.

dusk_functions.py: This file includes code snippets demonstrating the usage of the Dusk library. Dusk is a Python library for distributed computing and can be used for efficient and scalable big-data processing.

Feel free to explore these files and adapt the code to suit your specific requirements for data cleaning, MapReduce, and Dusk.

The Assignment 2 folder focuses on different search techniques and the application of divide-and-conquer algorithms for processing large datasets. It contains the following files:

sequential_search.py: This file contains code for implementing sequential search algorithms for big data processing. Sequential search is a basic search technique that iterates through a dataset sequentially to find specific items.

r_tree_search.py: Here, you will find code implementations of the R-tree search algorithm. R-trees are spatial indexing structures used for efficient search operations on multi-dimensional data, often applied to geographic data or in database systems.

divide_conquer_r_tree.py: This file includes code snippets demonstrating the usage of the divide-and-conquer approach for R-tree search. Divide and conquer algorithms break down a problem into smaller subproblems, solving them individually, and then combining the results.

Feel free to explore these files and adapt the code to suit your specific requirements for search operations and divide-and-conquer algorithms on large datasets.
